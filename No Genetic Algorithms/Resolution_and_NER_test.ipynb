{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5ffa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "# nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "\n",
    "# def resolve(corenlp_output):\n",
    "#     \"\"\" Transfer the word form of the antecedent to its associated pronominal anaphor(s) \"\"\"\n",
    "#     for coref in corenlp_output['corefs']:\n",
    "#         mentions = corenlp_output['corefs'][coref]\n",
    "#         antecedent = mentions[0]  # the antecedent is the first mention in the coreference chain\n",
    "#         for j in range(1, len(mentions)):\n",
    "#             mention = mentions[j]\n",
    "#             if mention['type'] == 'PRONOMINAL':\n",
    "#                 # get the attributes of the target mention in the corresponding sentence\n",
    "#                 target_sentence = mention['sentNum']\n",
    "#                 target_token = mention['startIndex'] - 1\n",
    "#                 # transfer the antecedent's word form to the appropriate token in the sentence\n",
    "#                 corenlp_output['sentences'][target_sentence - 1]['tokens'][target_token]['word'] = antecedent['text']\n",
    "\n",
    "\n",
    "# def print_resolved(corenlp_output):\n",
    "#     \"\"\" Print the \"resolved\" output \"\"\"\n",
    "#     possessives = ['hers', 'his', 'their', 'theirs']\n",
    "#     for sentence in corenlp_output['sentences']:\n",
    "#         for token in sentence['tokens']:\n",
    "#             output_word = token['word']\n",
    "#             # check lemmas as well as tags for possessive pronouns in case of tagging errors\n",
    "#             if token['lemma'] in possessives or token['pos'] == 'PRP$':\n",
    "#                 output_word += \"'s\"  # add the possessive morpheme\n",
    "#             output_word += token['after']\n",
    "#             print(output_word, end='')\n",
    "\n",
    "\n",
    "# text = \"The pen is mightier than the sword because it can only stab things.\"\n",
    "\n",
    "# output = nlp.annotate(text, properties= {'annotators':'dcoref','outputFormat':'json','ner.useSUTime':'false'})\n",
    "# output = json.loads(output)\n",
    "\n",
    "# resolve(output)\n",
    "\n",
    "# print('Original:', text)\n",
    "# print('\\nResolved: ', end='')\n",
    "# print_resolved(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e97e1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\srina\\AppData\\Local\\Temp\\tmprarcyztd\\config.json as plain json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8546e42144404f9b926e9304270e7106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5b0065832f4787b9640e65cc686795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb90136fdcf6447e824c403bb22553a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/634M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Spacy models 'en_core_web_sm' not found.  Downloading and installing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "model_url = \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\"\n",
    "predictor = Predictor.from_path(model_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ff904db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Google LLC is a multinational technology company. It was founded in September 1998 by Larry Page and Sergey Brin while they were PhD students at Stanford University in California.\n",
      "\n",
      "\n",
      "\n",
      "Coref resolved:  Google LLC is a multinational technology company. Google LLC was founded in September 1998 by Larry Page and Sergey Brin while Larry Page and Sergey Brin were PhD students at Stanford University in California.\n"
     ]
    }
   ],
   "source": [
    "text = \"Google LLC is a multinational technology company. It was founded in September 1998 by Larry Page and Sergey Brin while they were PhD students at Stanford University in California.\"\n",
    "\n",
    "prediction = predictor.predict(document=text)  # get prediction\n",
    "resolved = predictor.coref_resolved(text)\n",
    "\n",
    "print('Original: ', text)\n",
    "print('\\n\\n') #Newline\n",
    "print('Coref resolved: ', resolved)  # resolved text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4567dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "resolved = nlp(resolved)\n",
    "\n",
    "NER = []\n",
    "for X in resolved.ents:\n",
    "    NER.append((X.text, X.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f817407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('September 1998', 'DATE'),\n",
       " ('Larry Page', 'PERSON'),\n",
       " ('Sergey Brin', 'PERSON'),\n",
       " ('Larry Page', 'PERSON'),\n",
       " ('Sergey Brin', 'PERSON'),\n",
       " ('PhD', 'WORK_OF_ART'),\n",
       " ('Stanford University', 'ORG'),\n",
       " ('California', 'GPE')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712c757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumenv",
   "language": "python",
   "name": "sumenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
